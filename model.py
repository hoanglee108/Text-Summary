import pandas as pd
from datasets import Dataset
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq
import torch
import os
os.environ["WANDB_DISABLED"] = "true"

# Ki·ªÉm tra GPU
print(torch.cuda.is_available())   # Ph·∫£i l√† True
print(torch.cuda.get_device_name())  # Ph·∫£i in ra t√™n GPU c·ªßa b·∫°n

# Load d·ªØ li·ªáu
df = pd.read_csv('../path/to/your/data.csv').dropna()
dataset = Dataset.from_pandas(df)

# Chia train/validation (10% val)
dataset = dataset.train_test_split(test_size=0.1)
train_dataset = dataset["train"]
eval_dataset = dataset["test"]

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained("VietAI/vit5-base")

# Tokenize
max_input_length = 512
max_target_length = 128

def preprocess(example):
    input_text = "T√≥m t·∫Øt: " + example["vƒÉn b·∫£n"]
    target_text = example["t√≥m t·∫Øt"]

    # Tokenize input
    model_inputs = tokenizer(
        input_text,
        max_length=max_input_length,
        truncation=True,
        padding="max_length"
    )

    # Tokenize target (label)
    labels = tokenizer(
        target_text,
        max_length=max_target_length,
        truncation=True,
        padding="max_length"
    )["input_ids"]

    # Thay pad_token_id trong label b·∫±ng -100 ƒë·ªÉ kh√¥ng t√≠nh v√†o loss
    labels = [(label if label != tokenizer.pad_token_id else -100) for label in labels]
    model_inputs["labels"] = labels

    return model_inputs

# √Åp d·ª•ng ti·ªÅn x·ª≠ l√Ω
tokenized_train = train_dataset.map(preprocess, remove_columns=['vƒÉn b·∫£n', 't√≥m t·∫Øt'])
tokenized_eval = eval_dataset.map(preprocess, remove_columns=['vƒÉn b·∫£n', 't√≥m t·∫Øt'])

# Load model
model = AutoModelForSeq2SeqLM.from_pretrained("VietAI/vit5-base")

from datetime import datetime

training_args = TrainingArguments(
    output_dir="../output/dir",
    run_name=f"vit5-summary-{datetime.now().strftime('%Y%m%d-%H%M%S')}",
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    num_train_epochs=3,
    learning_rate=5e-5,
    weight_decay=0.01,
    save_total_limit=2,
    save_steps=500,
    logging_dir='./logs',
    logging_steps=100,
    fp16=True
)

# Hu·∫•n luy·ªán v·ªõi Trainer
data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_eval,
    data_collator=data_collator,
)

# B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán
trainer.train()

# üëâ L∆∞u m√¥ h√¨nh th·ªß c√¥ng sau khi hu·∫•n luy·ªán xong
trainer.save_model("./vit5_finetuned")